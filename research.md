---
title: Research
permalink: /Research/
header:
  overlay_image: unsplash-gallery-image-2.jpg
---

## Journal papers

- Krishnakumar Balasubramanian, Larry Goldstein, Nathan Ross and Adil Salim, [Gaussian random field approximation via Stein's method with applications to wide random neural networks](https://arxiv.org/pdf/2306.16308.pdf), _Applied and Computational Harmonic Analysis journal_, 2024.

- Adil Salim, [A Strong Law of Large Numbers for Random Monotone Operators](https://arxiv.org/abs/1910.04405), _Set-Valued and Variational Analysis_, 2023. 

- Adil Salim, Laurent Condat, Konstantin Mishchenko and Peter Richtárik, "[Dualize, Split, Randomize: Fast Nonsmooth Optimization Algorithms](https://arxiv.org/abs/2004.02635)", _Journal of Optimization Theory and Applications_, April 2020.

- Pascal Bianchi, Walid Hachem and Adil Salim, [A Fully Stochastic Primal-Dual Algorithm]({{site.baseurl}}{% link Research/pridu19.pdf %}), _Optimization Letters_, June 2020. 

- Adil Salim, Pascal Bianchi, and Walid Hachem, [Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs]({{site.baseurl}}{% link Research/snake18.pdf %}), _Transaction on Automatic Control_, March 2018. (Here is the [Code](https://github.com/adil-salim/Snake) and a [short version]({{site.baseurl}}{% link Research/snake-nips17.pdf %}) presented at _NIPS 2017 Workshop Black in AI_)

- Pascal Bianchi, Walid Hachem, and Adil Salim, [A constant step Forward-Backward algorithm involving random maximal monotone operators]({{site.baseurl}}{% link Research/joca1813_revised.pdf %}), _Journal of Convex Analysis_, March 2018. (Here is an extended [abstract]({{site.baseurl}}{% link Research/fb-short17.pdf %}))

- Pascal Bianchi, Walid Hachem, and Adil Salim, [Constant Step Stochastic Approximations Involving Differential Inclusions: Stability, Long-Run Convergence and Applications]({{site.baseurl}}{% link Research/revised_arxiv_dicst.pdf %}), _Stochastics_, May 2018. 

## Conference papers

- Victor Priser, Pascal Bianchi and Adil Salim, "[Long-time asymptotics of noisy SVGD outside the population limit](https://arxiv.org/pdf/2406.11929.pdf)", _ICLR 2025_.

- Marah Abdin *et al.*, "[Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219)", April 2024.

- Suriya Gunasekar, Yi Zhang *et al.*, "[Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)", June 2023.


- Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu and Adil Salim, "[The probability flow ODE is provably fast](https://arxiv.org/pdf/2305.11798.pdf)", _NeurIPS 2023_.

- Michael Diao, Krishnakumar Balasubramanian, Sinho Chewi, Adil Salim, "[Forward-Backward Gaussian Variational Inference
via JKO in the Bures–Wasserstein Space](https://arxiv.org/abs/2304.05398.pdf)", _ICML 2023_.

- Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim and Anru R. Zhang, "[Sampling is as easy as learning the score: theory for
diffusion models with minimal data assumptions](https://arxiv.org/pdf/2209.11215.pdf)", **Notable top 5% paper** @ _ICLR 2023_, Kigali, Rwanda.

- Sinho Chewi, Sébastien Bubeck and Adil Salim, "[On the complexity of finding stationary points of smooth functions in one dimension](https://arxiv.org/pdf/2209.07513.pdf)", **Best student paper award** @ _ALT 2023_, Singapore. 

- Lukang Sun, Adil Salim and Peter Richtárik, "[Federated Sampling with Langevin Algorithm under Isoperimetry](https://arxiv.org/pdf/2206.00920.pdf)",  _TMLR 2023_.

- Yongxin Chen, Sinho Chewi, Adil Salim, Andre Wibisono, "[Improved analysis for a proximal algorithm for sampling](https://arxiv.org/abs/2202.06386.pdf)", _COLT 2022_, London, UK.

- Krishnakumar Balasubramanian, Sinho Chewi, Murat A. Erdogdu, Adil Salim, Matthew Zhang, "[Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo](https://arxiv.org/abs/2202.05214.pdf)", _COLT 2022_, London, UK.

- Adil Salim, Lukang Sun, Peter Richtárik, "[A Convergence Theory for SVGD in the Population Limit under Talagrand’s Inequality T1](https://arxiv.org/pdf/2106.03076.pdf)", _ICML 2022_, Baltimore, USA.

- Adil Salim, Laurent Condat, Dmitry Kovalev and Peter Richtárik, "[An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints](https://arxiv.org/abs/2102.11079)", _AISTATS 2022_.

- Dmitry Kovalev, Adil Salim and Peter Richtárik, "[Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization](https://arxiv.org/abs/2006.11773)", _NeurIPS 2020_.

- Anna Korba, Adil Salim, Michael Arbel, Giulia Luise and Arthur Gretton, "[A Non-Asymptotic Analysis for Stein Variational Gradient Descent](https://arxiv.org/abs/2006.09797)", _NeurIPS 2020_. 

- Adil Salim and Peter Richtárik, "[Primal Dual Interpretation of the Proximal Stochastic Gradient Langevin Algorithm](https://arxiv.org/abs/2006.09270)", _NeurIPS 2020_. 

- Adil Salim, Anna Korba and Giulia Luise, "[The Wasserstein Proximal Gradient Algorithm](https://arxiv.org/abs/2002.03035)", _NeurIPS 2020_. 

- Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Adil Salim, Peter Richtárik and Martin Takáč, "[Distributed Fixed Point Methods with Compressed Iterates](https://arxiv.org/abs/1912.09925)", December 2019. 

- Sélim Chraibi, Adil Salim, Samuel Horváth, Filip Hanzely and Peter Richtárik, "Learning To Optimize Via Dual Space Preconditioning", Technical report, September 2019. 

- Adil Salim, Dmitry Kovalev and Peter Richtárik, "[Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates]({{site.baseurl}}{% link Research/langevin19.pdf %})", **Spotlight** @ _NeurIPS 2019_, Vancouver, Canada. (Here is the slides of the [Spotlight]({{site.baseurl}}{% link Research/spla-slides.pdf %}), the [Code](https://github.com/adil-salim/SPLA) and the [Poster]({{site.baseurl}}{% link Research/poster-langevin19.pdf %})).

- Michael Arbel, Anna Korba, Adil Salim and Arthur Gretton, "[Maximum Mean Discrepancy Gradient Flow](https://arxiv.org/abs/1906.04370)", _NeurIPS 2019_, Vancouver, Canada.

- Anna Korba, Adil Salim, Michael Arbel and Arthur Gretton, "Yet another look at Stein Variational Gradient Descent", _ICML 2019 Workshop on Stein's Method_, Long Beach, USA.

- Adil Salim and Walid Hachem, "[On the Performance of the Stochastic FISTA]({{site.baseurl}}{% link Research/fista19.pdf %})", March 2019. 

- Sholom Schechtman, Adil Salim, and Pascal Bianchi, "[Passty Langevin]({{site.baseurl}}{% link Research/cap2019.pdf %})", _CAp 2019_, Toulouse, France.

- Adil Salim, Pascal Bianchi, and Walid Hachem, "[A Constant Step Stochastic Douglas-Rachford Algorithm with Application to Non Separable Regularization]({{site.baseurl}}{% link Research/icassp18.pdf %})", _IEEE ICASSP 2018_, Calgary, Canada. (Here is the [Technical Report]({{site.baseurl}}{% link Research/TechnicalReport.pdf %}), an extended [abstract]({{site.baseurl}}{% link Research/stodr-short17.pdf %}) of the Technical Report and the [Code](https://github.com/adil-salim/Stochastic-DR))


- Adil Salim, Pascal Bianchi, and Walid Hachem, "[Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs]({{site.baseurl}}{% link Research/cap2017.pdf %})", _CAp 2017_, Grenoble, France.


- Pascal Bianchi, Walid Hachem, and Adil Salim, "[Convergence d'un algorithme du gradient proximal stochastique à pas constant et généralisation aux opérateurs monotones aléatoires]({{site.baseurl}}{% link Research/sbh_gretsi17.pdf %})", _GRETSI 2017_, Juan-les-Pins, France.


- Adil Salim, Pascal Bianchi, and Walid Hachem, "[A Stochastic Proximal Point Algorithm for Total Variation Regularization over Large Scale Graphs]({{site.baseurl}}{% link Research/proxtv.pdf %})", _IEEE CDC 2016_, Las Vegas, USA.


- Rahul Mourya, Pascal Bianchi, Adil Salim and Cédric Richard, "[An Adaptive Distributed Asynchronous Algorithm with Application to Target Localization]({{site.baseurl}}{% link Research/mourya2017adaptive.pdf %})", _IEEE CAMSAP 2017_, Curacao, Dutch Antilles. 

- Pascal Bianchi, Walid Hachem and Adil Salim, "Building Stochastic Optimization Algorithms with Random Monotone Operators", _EUCCO 2016_, Leuven, Belgium. 


## Reports
- PhD thesis, [Random monotone operators and application to stochastic optimization](
https://pastel.archives-ouvertes.fr/tel-01960496/document), under the supervision of [Pascal Bianchi](https://bianchi.wp.imt.fr/) and [Walid Hachem](http://www-syscom.univ-mlv.fr/~whachem/)

- Master's thesis, [Processus à accroissements libres]({{site.baseurl}}{% link Research/memoire-de-m2.pdf %}), under the supervision of [Philippe Biane](http://igm.univ-mlv.fr/~biane/)

- Intership report, [Méthodes séquentielles de traitement de données]({{site.baseurl}}{% link Research/rapport-de-stage.pdf %}), under the supervision of [Florian Maire](https://maths.ucd.ie/~fmaire/)

<!---

## Service

I have served as an area chair for:

- Black in AI workshop 2019, 2020

I have reviewed papers for:

- Journal of Machine Learning Research
- Journal of the Royal Statistical Society: Series B
- NeurIPS 2019, 2020
- ICML 2020
- ICLR 2021
- Set-Valued and Variational Analysis
- Applied Mathematics and Optimization
- IEEE Transactions on Information Theory
- IEEE Transactions on Signal and Information Processing over Networks
- IEEE Transactions on Signal Processing
- IEEE Signal Processing Letters
- Automatica
- Numerical Algorithms
- Journal of Mathematical Analysis and Applications
- Journal of Scientific Computing


## Awards and Distinctions

- **2020** Top 33% ICML reviewer
- **2019** Top 50% NeurIPS reviewer
- **2019** Participation to _MLSS 2019_, Skoltech Moscow, Russia
- **2019** Participation to _DS3 2019_, Ecole Polytechnique, France
- **2018** _GDR ISIS_ Travel Grant for PhD mobility at EPFL
- **2017, 2018** _NIPS Workshop Black in AI_ Travel Grant
- **2015** Top 3 Master's thesis out of nearly 200 students
- **2009 - 2012** Merit Scholarship

--->
